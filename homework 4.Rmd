---
title: "Homework 4"
author: "gzy"
date: "2024-11-29"
output: html_document
---

# 1. Introduction:
*1 paragraph introduction to the problem, which describes the importance of the issue, and briefly states what you will be doing in this report.*
Access to fresh and healthy food is a pressing issue in many American cities, leading to significant public health challenges and exacerbating food access inequities, particularly in underserved communities. In Philadelphia, the Food Trust has established farmers' markets across the city as a strategy to provide residents with fresh, nutritious food. However, some neighborhoods still face limited or no access to these markets, raising concerns about the equitable distribution of this vital resource. This report examines the spatial distribution of farmers' markets in Philadelphia using point pattern analysis to determine whether they are randomly placed, clustered, or dispersed, and to identify areas lacking access to these essential food sources.

# 2. Methodology:

## 2.1 What hypotheses you will be testing

*i.	What is CSR? Describe the conditions that need to hold in order for the point pattern to be completely spatially random.*
A point pattern is considered **Completely Spatially Random (CSR)** if the points within a defined area are distributed without any discernible pattern, and their placement is entirely random. CSR serves as a baseline model in spatial analysis, helping researchers determine whether a distribution is random, clustered, or dispersed. It is often used as a null hypothesis in point pattern analysis to assess deviations in spatial arrangements.

To establish that a point pattern is CSR, two critical conditions must be met. First, the probability of a point landing in any location within the study area must be directly proportional to the size of that location. If the area is divided into equal-sized cells, each cell should have an equal likelihood of containing a point. For example, if a region is divided into 54 cells of equal area, the probability of a point being in any cell would be uniform, with each cell having a likelihood of \(1/54\).

Second, the placement of one point must be entirely independent of the placement of other points. This means that the location of one point has no influence on where other points land, ensuring that no clustering or repulsion occurs. Together, these two conditions—equal probability of placement and independence of points—ensure that the point pattern is completely random. If either condition is violated, the pattern is likely not CSR, suggesting clustering or dispersion instead.

*ii.	What are your null/alternative hypotheses in point pattern analyses? *

In point pattern analyses, the null hypothesis (H₀) posits that the spatial distribution of points is CSR. This implies that points are equally likely to occur anywhere within the study area, with no preference for specific locations. Furthermore, the placement of one point does not influence the placement of others, ensuring independence of points. CSR assumes no clustering or systematic spacing between points, making it the default or baseline spatial pattern for comparison.

The alternative hypothesis (Hₐ), on the other hand, suggests that the spatial distribution of point deviates from CSR, indicating the presence of either clustering or dispersion. Clustering occurs when points are concentrated in specific areas, often reflecting an underlying attraction between points, such as hotspots of activity. Dispersion, in contrast, refers to points being more evenly spaced than expected under randomness, which might suggest repulsion or avoidance behaviors, such as territoriality.

## 2.2 The Quadrat method 

*i.	What does this method entail? That is, describe the method and be sure to write about it in your own words.*
The Quadrat Method is a spatial analysis technique used to study the distribution of points within a defined area. It involves dividing the study region into smaller, equally sized square cells called quadrats. The number of points (e.g., events, features, or observations) in each quadrat is then counted and analyzed to determine whether the points are randomly distributed, clustered, or evenly dispersed across the area.

*ii.	Discuss the limitations of the Quadrat method and why it’s generally not used in practice.*

This Method, while simple and widely used for basic spatial pattern analysis, has several significant limitations that make it less practical for more detailed or complex studies. One of the primary issues is its sensitivity to cell size. The choice of cell size greatly influences the results, as smaller cells may highlight variability and create many empty quadrats, while larger cells can obscure detailed patterns. This variability can lead to inconsistent or misleading interpretations, depending on the scale chosen.

Another major limitation is the method’s dependence on grid orientation and range. Slight changes in how the grid is placed over the study area or adjustments to the boundaries of the analysis can result in different outcomes, even when the underlying spatial pattern remains unchanged. This introduces a level of subjectivity, making it harder to ensure objectivity and reproducibility in the analysis.

Additionally, the quadrat method simplifies spatial data by converting the spatial distribution of points into counts within cells. This aggregation often leads to a loss of detailed spatial information. For example, two distinct patterns, such as clustered and evenly dispersed points, can produce identical quadrat counts, which limits the method’s ability to distinguish between these patterns effectively.

The method also suffers from the Modifiable Areal Unit Problem (MAUP), where the choice of how the space is divided into quadrats influences the results. This arbitrary division can introduce bias and reduce the reliability of findings. Combined with its inability to analyze multi-scale patterns or complex spatial relationships, the quadrat method is often overshadowed by more advanced techniques such as Ripley’s K-function or kernel density estimation, which provide deeper insights and are less susceptible to these issues.

In conclusion, while the quadrat method is useful for introductory spatial analysis due to its simplicity, its reliance on arbitrary decisions, loss of detail, and limited analytical power make it less practical for addressing modern spatial problems. More sophisticated methods are typically preferred for robust and detailed spatial analyses.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/eugeneby/Dropbox/Documents/Work and School/Teaching/CPLN 671 - Statistics and Data Mining/Point Pattern Analysis/Data/Quadrat Analysis Data")

#install.packages(c("sp", "ISLR", "MASS", "spatstat", "spatial", "maptools", "ppp", "fossil", "adehabitatHR", "gdata", "raster", "rgdal", "spatialEco", "spatstat.explore", "spatstat.model", "proxy", "sf"))

library(graphics)
library(maptools)
library(spatstat)
library(sp)
library(sf)
library(fossil)
library(spatial)
library(adehabitatHR)
library(gdata)
library(raster)
library(rgdal)
library(spatialEco)
library(spatstat.model)
library(spatstat.explore)
library(proxy)

knitr::opts_chunk$set(echo = TRUE)

options(scipen=999)
```


```{r warning=FALSE, message=FALSE, cache=FALSE}
Boundary <- rgdal::readOGR('.', 'Boundary')
BoundaryPolygons <- as(Boundary, "SpatialPolygons")
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")
plot(BoundaryPolygonsOW, main=NULL)
title(main = "Point Pattern Analysis")
```


```{r warning=FALSE, message=FALSE, cache=FALSE}
setwd("C:\\Users\\eugeneby\\Dropbox\\Documents\\Work and School\\Teaching\\CPLN 671 - Statistics and Data Mining\\Data\\Lecture 19 - 21 - R\\Point Pattern Analysis\\Data\\Quadrat Analysis Data")
Pts <- read.table("Quadrat Points.txt", header=T, sep="\t", colClasses = c("X"="double"))
pp <- ppp(Pts$X, Pts$Y, window=BoundaryPolygonsOW)
plot(pp, main=NULL)
title(main = "Point Pattern Analysis")
```


```{r}
# Generate the kernel density map of the points
plot(density(pp))
```


## 2.3 The Nearest Neighbor Analysis method

*i.	What does this method entail?  That is, describe the method and be sure to write about it in your own words. Be sure to specify the study area you are using here.*

*ii.	What statistical test will you use here? What is the test statistic? That is, how do you determine whether you have significant clustering or dispersion (uniformity)? Include the relevant formulas and be sure to describe them and all the terms that they include.*

*iii.	What are the limitations of the Nearest Neighbor Analysis? In particular, compare the problem at hand with the hospital example in the slides.*


```{r warning=FALSE, message=FALSE, cache=FALSE}
nnd <- nndist.ppp(pp)
#Average Observed Distance
MeanObsDist <- mean(nnd)  
#Average Expected Distance
MeanExpDist <- 0.5 / sqrt(nrow(Pts) / area.owin(BoundaryPolygonsOW))
#Standard Error
SE <- 0.26136 / sqrt(nrow(Pts)*nrow(Pts) / area.owin(BoundaryPolygonsOW))
```


Now, we can carry out a z-test to see whether the spatial distribution of our points is random, or whether there is significant clustering or dispersion. Recall that H~0~ states that we have randomness, and H~a~ states that we have either clustering (if NNI is significantly less than 1) or dispersion (if NNI is significantly greater than 1). Here, the z-statistic is calculated as follows:

$$ z= \frac{\overline{D}_O -\overline{D}_E }{SE_{\overline{D}_O}} $$

Based on the z-statistic, we can calculate the p-value. If z < 0, we do a lower-tailed test to test for clustering, and if z > 0, we do an upper-tailed test to test for dispersion. 

```{r warning=FALSE, message=FALSE, cache=FALSE}
zscore <- (MeanObsDist - MeanExpDist)/SE                    #Calculating the z-score
pval<-ifelse(zscore > 0, 1 - pnorm(zscore), pnorm(zscore))  #Calculating the p-value
zscore
pval
```

We can also easily calculate the NNI, simply as the ratio of the Average Observed Distance and Average Expected Distance.
```{r warning=FALSE, message=FALSE, cache=FALSE}
NNI <- MeanObsDist / MeanExpDist
NNI
```

As an aside, the ` spatialEco` package has an ` nni` function which unfortunately doesn't take user-defined study area files. We would have to covert our data to an ` sf` objects as shown below. However, the results that we get are pretty similar to what we got above, and the slight difference is due to the fact that the convex hull is used as the study area in the ` nni` function.

```{r warning=FALSE, message=FALSE, cache=FALSE}
xy <- Pts[,c(2,3)]      
sp.Pts <- st_as_sf(SpatialPointsDataFrame(coords=xy, data=Pts))   #Convert our data to a sf object
spatialEco::nni(sp.Pts,win="hull")                                #Results are still pretty similar to results above
```


```{r}
#spatialEco package has an nni function which unfortunately doesn't take 
#user-defined study area files
xy <- Pts[,c(2,3)]      
sp.Pts <- SpatialPointsDataFrame(coords=xy, data=Pts)   #Convert our data to a sp object
spatialEco::nni(sp.Pts,win="hull")      #Results are still pretty similar to results above

```


## 2.4 K-Functions Analysis method
i.	What does the method entail? That is, describe the method and be sure to write about it in your own words.
1.	Be sure to include what K(d) and L(d) functions are in this description, and how they are defined in the software package that you use.
ii.	Talk about the beginning and incremental distances, and specify how they are calculated.
iii.	Describe the testing procedure and the concept of confidence envelopes. 
iv.	What happens to points next to the border? Talk about Ripley’s Edge Correction and the Simulate Outer Boundary Values Edge Correction (and specify which one you use and why).
v.	Talk about the nonhomogeneous K-Functions – that is, situations when you need to take into account a reference measure such as population in your K-function analysis. 
1.	Briefly describe when/where this would be appropriate
2.	In your own words, describe how this may be done
a.	Hint: this is what is done in the last 25 or so slides.


```{r}
#K-FUNCTIONS
#What about K-functions? Of course we'll do K-functions as well!
#Setting working directory
setwd("C:/Users/Administrator/Dropbox/Documents/Work and School/Teaching/CPLN 671 - Statistics and Data Mining/Point Pattern Analysis/Data/K-Functions Taking Population Into Consideration")

#Reading Polygon Boundary.shp from the directory above
Boundary <- readOGR('.', 'PA_Albers')

#Class "SpatialPolygons" holds polygon topology (without attributes)
BoundaryPolygons <- as(Boundary, "SpatialPolygons")

#The class "owin" is a way of specifying the observation window for a point pattern.
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")

#Plotting the Boundary Window
#plot(r)
plot(BoundaryPolygonsOW, main="Point Pattern Analysis with K-Functions")#,add=T)

#Reading in the file with the points
Pts <- read.table("Hospitals_for_R.txt", header=T, sep="\t", colClasses = c("X"="double"))
#Very roughly speaking, using attach() in R is like relying on the implicit use of the 
#most recent data set. 
#http://www.r-bloggers.com/to-attach-or-not-attach-that-is-the-question/
#Only attach file once. If you do it more than once, use detach(Pts) command 1+ times
#to detach file
#attach(Pts)
#detach(Pts)
pp <- ppp(Pts$X, Pts$Y, window=BoundaryPolygonsOW)
#If the following error message is received: data contain duplicated points
#Use the command duplicated(X,Y) to see which points are duplicates.
#The command cbind(Pts,duplicated(X,Y)) will show you which points have duplicated values
#In general, unless you know that the duplicates shouldn't be there, you would ignore
#this warning.

#Now let's plot the points and the Boundary.
plot(pp,add=T)

```


```{r}
#http://www.math.umt.edu/graham/stat544/ripleys.pdf
#If we double click on the khat data set on the right, it will have 513 observations
#and 5 variables. We are interested in 2 of the variables: 
#-- r, which is the distance that goes in increments of 138.8693
#-- iso, which is the k-function calculated with Ripley's edge correction
#K-Functions
khat <-Kest(pp, rmax=250000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with
#line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
     main="Ripley's Estimated K-Function",
     cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2) 
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're
#looking at the lowest and highest values of the simulated envelopes. Here,
#alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat 
#library and not the boot library.
Kenv <- spatstat.core::envelope(pp,fun="Kest", rmax=250000, nsim=9, nrank=1) 
# Plots Ripley's K function with 99% simulation # envelopes, axis labels, and a title.
plot(Kenv,xlab="r",ylab="Khat(r)", cex.lab=1.6,cex.axis=1.5,main= 
       "Ripley's Khat with Confidence Envelopes",cex.main=1.5,lwd=2)
```


```{r}
#L-Functions
#Computes Ripley's L* for each sample event
lhat <- Lest(pp, rmax=250000) 
#Plots Ripley's L function calculated with line width 2, 
#Ripley's isotropic edge correction, with axis labels, and a main title.
plot(lhat$r,lhat$iso-lhat$r, xlab="r",ylab="Ripley's L",cex.lab=1.6,  
     cex.axis=1.5,cex.main=1.5,lty=1,lwd=2, main="Ripley's Estimated L-Function") 
#Overlays the theoretical L-function under CSR with a dashed (lty=8) line.
lines(lhat$r,lhat$theo-lhat$r,lty=8,lwd=2) 

#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're
#looking at the lowest and highest values of the simulated envelopes. Here,
#alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
Lenv <- spatstat.core::envelope(pp,fun="Lest", rmax=250000, nsim=9,nrank=1)
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(Lenv,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
     main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
#A better way to view this is to rotate this plot 45 degrees clockwise.
#Gives the Ripley's data frame a new name L2.
L2 <- Lenv 
#Now we will subtract the distance r from the R-defined Ripley's L's
#(This will be done for the observed L, theoretical L, lower and uper envelopes)
L2$obs <- L2$obs-L2$r
L2$theo <- L2$theo-L2$r
L2$lo <- L2$lo-L2$r
L2$hi <- L2$hi-L2$r
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(L2,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
     main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
```


# 3. Results

## 3.1 Present results of the Nearest Neighbor Analysis
```{r}

```

*i.	Describe the results. Do you reject the null hypothesis? *
n the Nearest Neighbor Analysis, the Nearest Neighbor Ratio (NNR) is 0.995357, which is extremely close to 1, indicating that the spatial distribution is highly likely to be random. The z-score, which measures how far the observed pattern deviates from randomness in terms of standard deviations, is -0.069945. This value is nearly zero, suggesting no significant deviation from randomness. The p-value is 0.944237, representing the probability of observing this pattern if the null hypothesis of randomness is true. Since the p-value is much greater than the commonly used threshold of 0.05, there is insufficient evidence to reject the null hypothesis.

The analysis strongly suggests that the spatial distribution of points is consistent with randomness. There is no statistically significant evidence to indicate clustering or dispersion in the pattern.

## 3.2 Present results of the K-function Analysis
```{r}

```

*i.	Describe the results, and be sure to specify what the beginning and incremental distances were, and why you set them at those levels.*

*1.	Do you reject the null hypothesis? At what distances?*

*ii.	Without actually doing the analyses, do you expect that the reason for not seeing any farmers markets in Northeast Philly and certain parts of North and South Philly is that the population in that part of the city is too low? That is, do you expect that you would get different results if you were to take population (say, at the zip code level) into consideration?*


# 4. Discussion
*a.	Are the results obtained with the Nearest Neighbor Analysis and K-function Analysis consistent with each other?*
*b.	Are they consistent with your expectations, based on the visual examination of the point data, and given the limitations of each method?*
*c.	Does it seem to be the case that areas with lower median incomes have fewer farmers markets? Without doing any statistical tests, present a map of the farmers markets overlaid on a map of median household income at the zip code level and discuss. The Philadelphia_ZipCodes shapefile has the MedIncome variable which is the median household income for the year 2000.*
*d.	What conclusions can you make based on these findings? That is, can you conclude that farmers markets are clustered in Philly? *
*e.	What might the policy implications of these findings be?*

