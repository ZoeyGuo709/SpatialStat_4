---
title: "Homework 4"
author: "gzy"
date: "2024-11-29"
output: html_document
---

# 1. Introduction:
*1 paragraph introduction to the problem, which describes the importance of the issue, and briefly states what you will be doing in this report.*
Access to fresh and healthy food is a pressing issue in many American cities, leading to significant public health challenges and exacerbating food access inequities, particularly in underserved communities. In Philadelphia, the Food Trust has established farmers' markets across the city as a strategy to provide residents with fresh, nutritious food. However, some neighborhoods still face limited or no access to these markets, raising concerns about the equitable distribution of this vital resource. This report examines the spatial distribution of farmers' markets in Philadelphia using point pattern analysis to determine whether they are randomly placed, clustered, or dispersed, and to identify areas lacking access to these essential food sources.

# 2. Methodology:

## 2.1 What hypotheses you will be testing

*i.	What is CSR? Describe the conditions that need to hold in order for the point pattern to be completely spatially random.*
A point pattern is considered **Completely Spatially Random (CSR)** if the points within a defined area are distributed without any discernible pattern, and their placement is entirely random. CSR serves as a baseline model in spatial analysis, helping researchers determine whether a distribution is random, clustered, or dispersed. It is often used as a null hypothesis in point pattern analysis to assess deviations in spatial arrangements.

To establish that a point pattern is CSR, two critical conditions must be met. First, the probability of a point landing in any location within the study area must be directly proportional to the size of that location. If the area is divided into equal-sized cells, each cell should have an equal likelihood of containing a point. For example, if a region is divided into 54 cells of equal area, the probability of a point being in any cell would be uniform, with each cell having a likelihood of \(1/54\).

Second, the placement of one point must be entirely independent of the placement of other points. This means that the location of one point has no influence on where other points land, ensuring that no clustering or repulsion occurs. Together, these two conditions—equal probability of placement and independence of points—ensure that the point pattern is completely random. If either condition is violated, the pattern is likely not CSR, suggesting clustering or dispersion instead.

*ii.	What are your null/alternative hypotheses in point pattern analyses? *

In point pattern analyses, the null hypothesis (H₀) posits that the spatial distribution of points is CSR. This implies that points are equally likely to occur anywhere within the study area, with no preference for specific locations. Furthermore, the placement of one point does not influence the placement of others, ensuring independence of points. CSR assumes no clustering or systematic spacing between points, making it the default or baseline spatial pattern for comparison.

The alternative hypothesis (Hₐ), on the other hand, suggests that the spatial distribution of point deviates from CSR, indicating the presence of either clustering or dispersion. Clustering occurs when points are concentrated in specific areas, often reflecting an underlying attraction between points, such as hotspots of activity. Dispersion, in contrast, refers to points being more evenly spaced than expected under randomness, which might suggest repulsion or avoidance behaviors, such as territoriality.

## 2.2 The Quadrat method 

i.	What does this method entail? That is, describe the method and be sure to write about it in your own words.
ii.	Discuss the limitations of the Quadrat method and why it’s generally not used in practice.

```{r setup}

knitr::opts_chunk$set(echo = FALSE)
options(scipen=999)
#https://www.e-education.psu.edu/geog586/l5_p9.html

install.packages("sp")
install.packages("ISLR")
install.packages("MASS")
install.packages("spatstat")
install.packages("spatial")
install.packages("maptools")
install.packages("ppp")
install.packages("fossil")
install.packages("adehabitatHR")
install.packages("gdata")
install.packages("raster")
install.packages("rgdal")
install.packages("geostatsp")
install.packages("spatialEco")
install.packages("spatstat.core")

#Install SpatStat
install.packages("D:/SpatialStat/HW 4/Package/spatstat.explore_3.3-3.tar.gz", repos = NULL)

#Install SpatStat
install.packages("D:/SpatialStat/HW 4/Package/maptools_1.1-6.tar.gz", repos = NULL)
```


```{r}
library(graphics)
library(maptools)
library(spatstat)
library(sp)
library(fossil)
library(spatial)
library(adehabitatHR)
library(gdata)
library(raster)
library(rgdal)
library(geostatsp)
library(spatialEco)
library(spatstat.core)
```


```{r}
#Setting working directory
setwd("D:/SpatialStat/K-Function/0. Quadrat Analysis Data/Quadrat Analysis Data")

#Reading Polygon Boundary.shp from the directory above
Boundary <- rgdal::readOGR(dsn = "D:/SpatialStat/K-Function/0. Quadrat Analysis Data/Quadrat Analysis Data", layer = "Boundary")


#Class "SpatialPolygons" holds polygon topology (without attributes)
BoundaryPolygons <- as(Boundary, "SpatialPolygons")

#The class "owin" is a way of specifying the observation window for a point pattern.
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")

#Plotting the Boundary Window
plot(BoundaryPolygonsOW, main=NULL)
title(main = "Point Pattern Analysis")
```


```{r}
#Reading in the file with the points
Pts <- read.table("Quadrat Points.txt", header=T, sep="\t", colClasses = c("X"="double"))
#Very roughly speaking, using attach() in R is like relying on the implicit use of the 
#most recent data set. 
#http://www.r-bloggers.com/to-attach-or-not-attach-that-is-the-question/
#Only attach file once. If you do it more than once, use detach(Pts) command 1+ times
#to detach file
#attach(Pts)
#detach(Pts)
pp <- ppp(Pts$X, Pts$Y, window=BoundaryPolygonsOW)
#If the following error message is received: data contain duplicated points
#Use the command duplicated(X,Y) to see which points are duplicates.
#The command cbind(Pts,duplicated(X,Y)) will show you which points have duplicated values
#In general, unless you know that the duplicates shouldn't be there, you would ignore
#this warning.

#Now let's plot the points and the Boundary.
plot(pp, main=NULL)
title(main = "Point Pattern Analysis")
```

```{r}
#First let's create a matrix that only includes the x & y coordinates of each point.
#This is simply the last two columns of matrix Pts.
xy <-Pts[,2:3]

#In order for R to consider xy a spatial object, we need to convert it to 
#class SpatialPoints. We do it like this:
xysp <-SpatialPoints(xy)

#Now, let's use the mcp (minimum convex polygon) function to generate the mcp and
#plot the points with the mcp
#If we wanted to exclude some outlier points (e.g., 5 of the 100 points in the data)
#we would specify a different percent value below (e.g., 95 instead of 100)
cp <- mcp(xysp, percent=100)

#The class "owin" is a way of specifying the observation window for a point pattern.
cpnew <- as(cp, "owin")

#Here, we are using the attached dataset Pts to plot X and Y coordinates within the
#window cpnew.
mcpoly <- ppp(Pts$X,Pts$Y, window=cpnew)
plot(mcpoly, main=NULL)


#A few pretty cool things we can do with the data:
#1. Generate the kernel density map of the points
plot(density(pp))
#2. Generate the countour map. The add=TRUE (or add=T) is a way for R to know that
#you want the new plot to be added onto the existing plot.
contour(density(pp), add=TRUE)
#3. Add points on top of the maps
plot(pp, add=T)

```


## 2.3 The Nearest Neighbor Analysis method
i.	What does this method entail?  That is, describe the method and be sure to write about it in your own words. Be sure to specify the study area you are using here.
ii.	What statistical test will you use here? What is the test statistic? That is, how do you determine whether you have significant clustering or dispersion (uniformity)? Include the relevant formulas and be sure to describe them and all the terms that they include.
iii.	What are the limitations of the Nearest Neighbor Analysis? In particular, compare the problem at hand with the hospital example in the slides.

```{r}
#NEAREST NEIGHBOR ANALYSIS
#Now let's try some Nearest Neighbor Analysis!
dev.off() 
#Computes the distance from each point to its nearest neighbour in a point pattern.
nnd <- nndist.ppp(pp)
#Using the formulas on the slides, we calculate Mean Observed Distance,
#Mean Expected Distance and the Standard Error.
MeanObsDist <- mean(nnd)
#The area.owin command calculates the area of the study area that you use.
#Here it's the minimum enclosing rectangle, but it doesn't have to be - it 
#could be any shapefile you import from ArcGIS (or generate in R) that 
#corresponds to the study area.
MeanExpDist <- 0.5 / sqrt(nrow(Pts) / area.owin(BoundaryPolygonsOW))
SE <- 0.26136 / sqrt(nrow(Pts)*nrow(Pts) / area.owin(BoundaryPolygonsOW))
#Calculating the z-score
zscore <- (MeanObsDist - MeanExpDist)/SE
#Statistical test
#Here, if the z score is positive, we do an upper-tailed test and if the 
#z score is negative we do a lower-tailed test to come up with the p-value.
pval<-ifelse(zscore > 0, 1 - pnorm(zscore), pnorm(zscore))
#Calculating the NNI
NNI <- MeanObsDist / MeanExpDist
pval
NNI
```


```{r}
#spatialEco package has an nni function which unfortunately doesn't take 
#user-defined study area files
xy <- Pts[,c(2,3)]      
sp.Pts <- SpatialPointsDataFrame(coords=xy, data=Pts)   #Convert our data to a sp object
spatialEco::nni(sp.Pts,win="hull")      #Results are still pretty similar to results above

```


## 2.4 K-Functions Analysis method
i.	What does the method entail? That is, describe the method and be sure to write about it in your own words.
1.	Be sure to include what K(d) and L(d) functions are in this description, and how they are defined in the software package that you use.
ii.	Talk about the beginning and incremental distances, and specify how they are calculated.
iii.	Describe the testing procedure and the concept of confidence envelopes. 
iv.	What happens to points next to the border? Talk about Ripley’s Edge Correction and the Simulate Outer Boundary Values Edge Correction (and specify which one you use and why).
v.	Talk about the nonhomogeneous K-Functions – that is, situations when you need to take into account a reference measure such as population in your K-function analysis. 
1.	Briefly describe when/where this would be appropriate
2.	In your own words, describe how this may be done
a.	Hint: this is what is done in the last 25 or so slides.


```{r}
#K-FUNCTIONS
#What about K-functions? Of course we'll do K-functions as well!
#Setting working directory
setwd("C:/Users/Administrator/Dropbox/Documents/Work and School/Teaching/CPLN 671 - Statistics and Data Mining/Point Pattern Analysis/Data/K-Functions Taking Population Into Consideration")

#Reading Polygon Boundary.shp from the directory above
Boundary <- readOGR('.', 'PA_Albers')

#Class "SpatialPolygons" holds polygon topology (without attributes)
BoundaryPolygons <- as(Boundary, "SpatialPolygons")

#The class "owin" is a way of specifying the observation window for a point pattern.
BoundaryPolygonsOW<- as(BoundaryPolygons, "owin")

#Plotting the Boundary Window
#plot(r)
plot(BoundaryPolygonsOW, main="Point Pattern Analysis with K-Functions")#,add=T)

#Reading in the file with the points
Pts <- read.table("Hospitals_for_R.txt", header=T, sep="\t", colClasses = c("X"="double"))
#Very roughly speaking, using attach() in R is like relying on the implicit use of the 
#most recent data set. 
#http://www.r-bloggers.com/to-attach-or-not-attach-that-is-the-question/
#Only attach file once. If you do it more than once, use detach(Pts) command 1+ times
#to detach file
#attach(Pts)
#detach(Pts)
pp <- ppp(Pts$X, Pts$Y, window=BoundaryPolygonsOW)
#If the following error message is received: data contain duplicated points
#Use the command duplicated(X,Y) to see which points are duplicates.
#The command cbind(Pts,duplicated(X,Y)) will show you which points have duplicated values
#In general, unless you know that the duplicates shouldn't be there, you would ignore
#this warning.

#Now let's plot the points and the Boundary.
plot(pp,add=T)

```


```{r}
#http://www.math.umt.edu/graham/stat544/ripleys.pdf
#If we double click on the khat data set on the right, it will have 513 observations
#and 5 variables. We are interested in 2 of the variables: 
#-- r, which is the distance that goes in increments of 138.8693
#-- iso, which is the k-function calculated with Ripley's edge correction
#K-Functions
khat <-Kest(pp, rmax=250000) #,correction="Ripley")
#Plots Ripley's K function calculated with Ripley's isotropic edge correction, with
#line width 2, axis labels, and a main title.
plot(khat$r,khat$iso,xlab="r", ylab="Ripley's K",
     main="Ripley's Estimated K-Function",
     cex.lab=1.6,cex.axis=1.5,cex.main=1.5,lty=1,lwd=2)
# Overlays the theoretical K-function under CSR with a dashed (lty=8) line.
lines(khat$r,khat$theo,lty=8, lwd=2) 
#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're
#looking at the lowest and highest values of the simulated envelopes. Here,
#alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
#spatstat::envelope is to specify that the envelope command is in the spatstat 
#library and not the boot library.
Kenv <- spatstat.core::envelope(pp,fun="Kest", rmax=250000, nsim=9, nrank=1) 
# Plots Ripley's K function with 99% simulation # envelopes, axis labels, and a title.
plot(Kenv,xlab="r",ylab="Khat(r)", cex.lab=1.6,cex.axis=1.5,main= 
       "Ripley's Khat with Confidence Envelopes",cex.main=1.5,lwd=2)
```


```{r}
#L-Functions
#Computes Ripley's L* for each sample event
lhat <- Lest(pp, rmax=250000) 
#Plots Ripley's L function calculated with line width 2, 
#Ripley's isotropic edge correction, with axis labels, and a main title.
plot(lhat$r,lhat$iso-lhat$r, xlab="r",ylab="Ripley's L",cex.lab=1.6,  
     cex.axis=1.5,cex.main=1.5,lty=1,lwd=2, main="Ripley's Estimated L-Function") 
#Overlays the theoretical L-function under CSR with a dashed (lty=8) line.
lines(lhat$r,lhat$theo-lhat$r,lty=8,lwd=2) 

#Code to compute the Ripley's Simulation Confidence Envelopes
#Computes confidence envelopes using n=199 simulations. Here, nrank=1 means we're
#looking at the lowest and highest values of the simulated envelopes. Here,
#alpha = 2 * nrank/(1 + nsim) = 2*1/200 = 0.01
Lenv <- spatstat.core::envelope(pp,fun="Lest", rmax=250000, nsim=9,nrank=1)
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(Lenv,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
     main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
#A better way to view this is to rotate this plot 45 degrees clockwise.
#Gives the Ripley's data frame a new name L2.
L2 <- Lenv 
#Now we will subtract the distance r from the R-defined Ripley's L's
#(This will be done for the observed L, theoretical L, lower and uper envelopes)
L2$obs <- L2$obs-L2$r
L2$theo <- L2$theo-L2$r
L2$lo <- L2$lo-L2$r
L2$hi <- L2$hi-L2$r
# Plots Ripley's L function with 99% simulation envelopes, axis labels, and a title.
plot(L2,xlab="r",ylab="Lhat(r)", cex.lab=1.6,cex.axis=1.5,
     main= "Ripley's Lhat with Confidence Envelopes",cex.main=1.5,lwd=2,legend=F)
```


# 3. Results

## 3.1 Present results of the Nearest Neighbor Analysis
i.	Describe the results. Do you reject the null hypothesis? 

## 3.2 Present results of the K-function Analysis
i.	Describe the results, and be sure to specify what the beginning and incremental distances were, and why you set them at those levels.
1.	Do you reject the null hypothesis? At what distances?
ii.	Without actually doing the analyses, do you expect that the reason for not seeing any farmers markets in Northeast Philly and certain parts of North and South Philly is that the population in that part of the city is too low? That is, do you expect that you would get different results if you were to take population (say, at the zip code level) into consideration?


# 4. Discussion
a.	Are the results obtained with the Nearest Neighbor Analysis and K-function Analysis consistent with each other?
b.	Are they consistent with your expectations, based on the visual examination of the point data, and given the limitations of each method?
c.	Does it seem to be the case that areas with lower median incomes have fewer farmers markets? Without doing any statistical tests, present a map of the farmers markets overlaid on a map of median household income at the zip code level and discuss. The Philadelphia_ZipCodes shapefile has the MedIncome variable which is the median household income for the year 2000.
d.	What conclusions can you make based on these findings? That is, can you conclude that farmers markets are clustered in Philly? 
e.	What might the policy implications of these findings be?

